# Databricks notebook source
# MAGIC %md
# MAGIC # NYC Yellow Trip Records - Camada Bronze (Ingest√£o)
# MAGIC 
# MAGIC **Objetivo:** Ingerir dados brutos dos Yellow Trip Records de NYC (jan-abr/2023) na camada Bronze
# MAGIC 
# MAGIC **Fonte:** https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page
# MAGIC 
# MAGIC **Caracter√≠sticas da Camada Bronze:**
# MAGIC - Dados brutos, sem transforma√ß√µes
# MAGIC - Preserva√ß√£o da estrutura original
# MAGIC - Armazenamento em formato Delta Lake
# MAGIC - Particionamento por data de pickup
# MAGIC - Controle de qualidade b√°sico

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. Configura√ß√£o e Imports

# COMMAND ----------

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import logging
from datetime import datetime
import os

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. Configura√ß√µes do Ambiente

# COMMAND ----------

# Configura√ß√µes do projeto
PROJECT_NAME = "nyc_yellow_trip"
BRONZE_TABLE = "bronze.yellow_tripdata"
DATA_PATH = "/FileStore/shared_uploads/data/yellow_tripdata_2023/"
DELTA_PATH_BRONZE = "/delta/bronze/yellow_tripdata/"

# URLs dos dados (jan-abr 2023)
DATA_URLS = {
    "2023-01": "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet",
    "2023-02": "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet",
    "2023-03": "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-03.parquet",
    "2023-04": "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-04.parquet"
}

# Schema esperado dos dados
EXPECTED_SCHEMA = StructType([
    StructField("VendorID", IntegerType(), True),
    StructField("tpep_pickup_datetime", TimestampType(), True),
    StructField("tpep_dropoff_datetime", TimestampType(), True),
    StructField("passenger_count", DoubleType(), True),
    StructField("trip_distance", DoubleType(), True),
    StructField("RatecodeID", DoubleType(), True),
    StructField("store_and_fwd_flag", StringType(), True),
    StructField("PULocationID", IntegerType(), True),
    StructField("DOLocationID", IntegerType(), True),
    StructField("payment_type", IntegerType(), True),
    StructField("fare_amount", DoubleType(), True),
    StructField("extra", DoubleType(), True),
    StructField("mta_tax", DoubleType(), True),
    StructField("tip_amount", DoubleType(), True),
    StructField("tolls_amount", DoubleType(), True),
    StructField("improvement_surcharge", DoubleType(), True),
    StructField("total_amount", DoubleType(), True),
    StructField("congestion_surcharge", DoubleType(), True),
    StructField("airport_fee", DoubleType(), True)
])

print("Configura√ß√µes carregadas")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Fun√ß√µes Auxiliares

# COMMAND ----------

def download_data_if_needed(url: str, local_path: str) -> bool:
    """
    Baixa dados se n√£o existirem localmente
    """
    try:
        # Verifica se o arquivo j√° existe
        if os.path.exists(local_path):
            logger.info(f"Arquivo j√° existe: {local_path}")
            return True
            
        # Para ambiente Databricks, usamos dbutils para download
        dbutils.fs.cp(url, local_path)
        logger.info(f"Arquivo baixado: {url} -> {local_path}")
        return True
        
    except Exception as e:
        logger.error(f"Erro ao baixar {url}: {str(e)}")
        return False

def validate_data_quality(df, month_year: str) -> dict:
    """
    Valida√ß√£o b√°sica de qualidade dos dados
    """
    total_rows = df.count()
    
    # Contagem de nulos nas colunas cr√≠ticas
    critical_columns = ["tpep_pickup_datetime", "tpep_dropoff_datetime", "trip_distance", "total_amount"]
    null_counts = {}
    
    for col in critical_columns:
        if col in df.columns:
            null_count = df.filter(df[col].isNull()).count()
            null_counts[col] = null_count
    
    # Valida√ß√µes adicionais
    negative_amounts = df.filter(col("total_amount") < 0).count()
    zero_distance = df.filter(col("trip_distance") == 0).count()
    
    quality_report = {
        "month_year": month_year,
        "total_rows": total_rows,
        "null_counts": null_counts,
        "negative_amounts": negative_amounts,
        "zero_distance_trips": zero_distance,
        "timestamp": datetime.now().isoformat()
    }
    
    return quality_report

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. Ingest√£o dos Dados

# COMMAND ----------

# Criar database se n√£o existir
spark.sql("CREATE DATABASE IF NOT EXISTS bronze")

print("Iniciando ingest√£o dos dados...")

# Lista para armazenar relat√≥rios de qualidade
quality_reports = []

# Processar cada m√™s
for month_year, url in DATA_URLS.items():
    print(f"\nProcessando dados de {month_year}...")
    
    try:
        # Ler dados diretamente da URL (formato Parquet)
        df = spark.read.parquet(url)
        
        # Adicionar colunas de metadados
        df_with_metadata = df.withColumn("ingestion_timestamp", current_timestamp()) \
                            .withColumn("source_file", lit(f"yellow_tripdata_{month_year}.parquet")) \
                            .withColumn("ingestion_month", lit(month_year))
        
        # Valida√ß√£o de qualidade
        quality_report = validate_data_quality(df, month_year)
        quality_reports.append(quality_report)
        
        print(f"   {quality_report['total_rows']:,} registros carregados")
        print(f"   Nulos em colunas cr√≠ticas: {quality_report['null_counts']}")
        
        # Escrever na camada Bronze (modo append para acumular os meses)
        if month_year == "2023-01":  # Primeiro m√™s - sobrescrever
            write_mode = "overwrite"
        else:  # Demais meses - adicionar
            write_mode = "append"
            
        df_with_metadata.write \
            .mode(write_mode) \
            .option("mergeSchema", "true") \
            .partitionBy("ingestion_month") \
            .format("delta") \
            .saveAsTable(BRONZE_TABLE)
            
        print(f"   Dados salvos na tabela {BRONZE_TABLE}")
        
    except Exception as e:
        logger.error(f"Erro ao processar {month_year}: {str(e)}")
        continue

print("\nIngest√£o conclu√≠da!")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5. Verifica√ß√£o e Relat√≥rio Final

# COMMAND ----------

# Verificar tabela criada
bronze_df = spark.table(BRONZE_TABLE)

print("RELAT√ìRIO DE INGEST√ÉO - CAMADA BRONZE")
print("=" * 50)

# Estat√≠sticas gerais
total_records = bronze_df.count()
print(f"Total de registros: {total_records:,}")

# Contagem por m√™s
monthly_counts = bronze_df.groupBy("ingestion_month").count().orderBy("ingestion_month")
print("\nRegistros por m√™s:")
monthly_counts.show()

# Per√≠odo dos dados
date_range = bronze_df.agg(
    min("tpep_pickup_datetime").alias("min_date"),
    max("tpep_pickup_datetime").alias("max_date")
).collect()[0]

print(f"\nüìÜ Per√≠odo dos dados:")
print(f"   In√≠cio: {date_range['min_date']}")
print(f"   Fim: {date_range['max_date']}")

# Schema da tabela
print(f"\nSchema da tabela:")
bronze_df.printSchema()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 6. Relat√≥rio de Qualidade Detalhado

# COMMAND ----------

print("RELAT√ìRIO DE QUALIDADE DOS DADOS")
print("=" * 40)

for report in quality_reports:
    print(f"\n{report['month_year']}:")
    print(f"   Registros: {report['total_records']:,}")
    print(f"   Valores negativos em total_amount: {report['negative_amounts']:,}")
    print(f"   Viagens com dist√¢ncia zero: {report['zero_distance_trips']:,}")
    
    if report['null_counts']:
        print("   Valores nulos:")
        for col, null_count in report['null_counts'].items():
            percentage = (null_count / report['total_records']) * 100
            print(f"     {col}: {null_count:,} ({percentage:.2f}%)")

# An√°lise adicional de qualidade
print("\nAN√ÅLISES ADICIONAIS:")

# Distribui√ß√£o por vendor
vendor_dist = bronze_df.groupBy("VendorID").count().orderBy("VendorID")
print("\nDistribui√ß√£o por Vendor:")
vendor_dist.show()

# Estat√≠sticas de valores monet√°rios
monetary_stats = bronze_df.select(
    avg("fare_amount").alias("avg_fare"),
    avg("tip_amount").alias("avg_tip"),
    avg("total_amount").alias("avg_total"),
    max("total_amount").alias("max_total"),
    min("total_amount").alias("min_total")
).collect()[0]

print("Estat√≠sticas monet√°rias:")
print(f"   Tarifa m√©dia: ${monetary_stats['avg_fare']:.2f}")
print(f"   Gorjeta m√©dia: ${monetary_stats['avg_tip']:.2f}")
print(f"   Total m√©dio: ${monetary_stats['avg_total']:.2f}")
print(f"   Total m√°ximo: ${monetary_stats['max_total']:.2f}")
print(f"   Total m√≠nimo: ${monetary_stats['min_total']:.2f}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 7. Otimiza√ß√£o da Tabela Delta

# COMMAND ----------

# Otimizar tabela Delta
print("Otimizando tabela Delta...")

# OPTIMIZE e Z-ORDER
spark.sql(f"""
OPTIMIZE {BRONZE_TABLE}
ZORDER BY (tpep_pickup_datetime, VendorID)
""")

# Vacuum (limpar vers√µes antigas - cuidado em produ√ß√£o)
# spark.sql(f"VACUUM {BRONZE_TABLE} RETAIN 168 HOURS")  # 7 dias

print("Otimiza√ß√£o conclu√≠da!")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 8. Metadados e Documenta√ß√£o

# COMMAND ----------

# Informa√ß√µes sobre a tabela
print("INFORMA√á√ïES DA TABELA BRONZE")
print("=" * 35)

# Mostrar informa√ß√µes da tabela
spark.sql(f"DESCRIBE EXTENDED {BRONZE_TABLE}").show(50, truncate=False)

# Hist√≥rico da tabela Delta
print("\nüìö Hist√≥rico da tabela Delta:")
spark.sql(f"DESCRIBE HISTORY {BRONZE_TABLE}").show(10, truncate=False)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Resumo da Camada Bronze
# MAGIC 
# MAGIC ### Dados Ingeridos:
# MAGIC - **Per√≠odo:** Janeiro a Abril de 2023
# MAGIC - **Fonte:** NYC TLC Trip Record Data (formato Parquet)
# MAGIC - **Armazenamento:** Delta Lake particionado por m√™s de ingest√£o
# MAGIC - **Schema:** Preservado conforme dados originais
# MAGIC 
# MAGIC ### Controles de Qualidade:
# MAGIC - Valida√ß√£o de valores nulos em colunas cr√≠ticas
# MAGIC - Identifica√ß√£o de valores negativos e inconsist√™ncias
# MAGIC - Relat√≥rio detalhado por m√™s de ingest√£o
# MAGIC - Metadados de ingest√£o adicionados
# MAGIC 
# MAGIC ### Pr√≥ximos Passos:
# MAGIC 1. Executar notebook `02_tratamento_silver.py` para limpeza e tratamento
# MAGIC 2. Aplicar regras de neg√≥cio e valida√ß√µes adicionais
# MAGIC 3. Criar camada Silver com dados limpos e estruturados
# MAGIC 
# MAGIC ### Localiza√ß√£o dos Dados:
# MAGIC - **Tabela:** `bronze.yellow_tripdata`
# MAGIC - **Particionamento:** `ingestion_month`
# MAGIC - **Formato:** Delta Lake com otimiza√ß√µes ZORDER

# COMMAND ----------

print("INGEST√ÉO BRONZE CONCLU√çDA COM SUCESSO!")
print("Pr√≥ximo passo: Execute o notebook 02_tratamento_silver.py")
