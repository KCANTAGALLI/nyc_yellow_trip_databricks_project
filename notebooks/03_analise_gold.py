# Databricks notebook source
# MAGIC %md
# MAGIC # NYC Yellow Trip Records - Camada Gold (An√°lise e M√©tricas)
# MAGIC 
# MAGIC **Objetivo:** Criar m√©tricas de neg√≥cio, KPIs e insights anal√≠ticos a partir dos dados limpos da camada Silver
# MAGIC 
# MAGIC **Caracter√≠sticas da Camada Gold:**
# MAGIC - M√©tricas agregadas e KPIs de neg√≥cio
# MAGIC - Tabelas dimensionais e de fatos
# MAGIC - Dados prontos para consumo por dashboards e relat√≥rios
# MAGIC - Insights e an√°lises avan√ßadas
# MAGIC - Performance otimizada para consultas anal√≠ticas

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. Configura√ß√£o e Imports

# COMMAND ----------

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import logging
from datetime import datetime, timedelta

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configura√ß√£o para gr√°ficos
plt.style.use('default')
sns.set_palette("husl")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. Configura√ß√µes e Defini√ß√µes

# COMMAND ----------

# Configura√ß√µes do projeto
SILVER_TABLE = "silver.yellow_tripdata_clean"
GOLD_DATABASE = "gold"

# Tabelas Gold a serem criadas
GOLD_TABLES = {
    "trip_metrics_daily": "gold.yellow_trip_metrics_daily",
    "trip_metrics_hourly": "gold.yellow_trip_metrics_hourly",
    "location_metrics": "gold.yellow_trip_location_metrics",
    "vendor_performance": "gold.yellow_trip_vendor_performance",
    "payment_analysis": "gold.yellow_trip_payment_analysis",
    "time_series_metrics": "gold.yellow_trip_time_series",
    "route_analysis": "gold.yellow_trip_route_analysis",
    "financial_summary": "gold.yellow_trip_financial_summary"
}

# KPIs principais para tracking
MAIN_KPIS = [
    "total_trips", "total_revenue", "avg_trip_distance", 
    "avg_trip_duration", "avg_fare_amount", "avg_tip_percentage",
    "total_passengers", "avg_speed"
]

print("Configura√ß√µes carregadas")
print(f"Tabelas Gold a serem criadas: {len(GOLD_TABLES)}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Carregamento dos Dados Silver

# COMMAND ----------

print("Carregando dados da camada Silver...")

# Carregar dados limpos da camada Silver
silver_df = spark.table(SILVER_TABLE)

print(f"Registros na camada Silver: {silver_df.count():,}")

# Verificar per√≠odo dos dados
date_range = silver_df.agg(
    min("tpep_pickup_datetime").alias("min_date"),
    max("tpep_pickup_datetime").alias("max_date"),
    count("*").alias("total_records")
).collect()[0]

print(f"Per√≠odo: {date_range['min_date']} a {date_range['max_date']}")
print(f"Total de registros: {date_range['total_records']:,}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. Cria√ß√£o do Database Gold

# COMMAND ----------

# Criar database Gold
spark.sql(f"CREATE DATABASE IF NOT EXISTS {GOLD_DATABASE}")
print(f"Database {GOLD_DATABASE} criado/verificado")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5. M√©tricas Di√°rias (Tabela Principal de KPIs)

# COMMAND ----------

print("Criando m√©tricas di√°rias...")

# M√©tricas agregadas por dia
daily_metrics = silver_df.groupBy(
    date_format("tpep_pickup_datetime", "yyyy-MM-dd").alias("trip_date"),
    "pickup_year",
    "pickup_month",
    dayofmonth("tpep_pickup_datetime").alias("pickup_day"),
    "pickup_dayname",
    "is_weekend"
).agg(
    # Volume de viagens
    count("*").alias("total_trips"),
    countDistinct("VendorID").alias("active_vendors"),
    
    # M√©tricas financeiras
    sum("total_amount").alias("total_revenue"),
    sum("fare_amount").alias("total_fare"),
    sum("tip_amount").alias("total_tips"),
    sum("tolls_amount").alias("total_tolls"),
    avg("total_amount").alias("avg_total_amount"),
    avg("fare_amount").alias("avg_fare_amount"),
    avg("tip_amount").alias("avg_tip_amount"),
    avg("tip_percentage").alias("avg_tip_percentage"),
    
    # M√©tricas operacionais
    sum("trip_distance").alias("total_distance"),
    avg("trip_distance").alias("avg_trip_distance"),
    sum("trip_duration_minutes").alias("total_duration_minutes"),
    avg("trip_duration_minutes").alias("avg_trip_duration"),
    avg("avg_speed_mph").alias("avg_speed"),
    
    # Passageiros
    sum("passenger_count").alias("total_passengers"),
    avg("passenger_count").alias("avg_passengers_per_trip"),
    
    # Qualidade dos dados
    sum(when(col("has_outlier"), 1).otherwise(0)).alias("trips_with_outliers"),
    
    # Distribui√ß√£o por per√≠odo
    sum(when(col("time_period") == "Morning", 1).otherwise(0)).alias("morning_trips"),
    sum(when(col("time_period") == "Afternoon", 1).otherwise(0)).alias("afternoon_trips"),
    sum(when(col("time_period") == "Evening", 1).otherwise(0)).alias("evening_trips"),
    sum(when(col("time_period") == "Night", 1).otherwise(0)).alias("night_trips")
).withColumn(
    # M√©tricas derivadas
    "revenue_per_mile", col("total_revenue") / col("total_distance")
).withColumn(
    "outlier_percentage", (col("trips_with_outliers") / col("total_trips")) * 100
).withColumn(
    "processing_timestamp", current_timestamp()
).orderBy("trip_date")

# Salvar tabela de m√©tricas di√°rias
daily_metrics.write \
    .mode("overwrite") \
    .format("delta") \
    .saveAsTable(GOLD_TABLES["trip_metrics_daily"])

print(f"Tabela criada: {GOLD_TABLES['trip_metrics_daily']}")
print(f"   {daily_metrics.count()} dias de dados")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 6. M√©tricas Hor√°rias (An√°lise de Padr√µes Temporais)

# COMMAND ----------

print("Criando m√©tricas hor√°rias...")

# M√©tricas por hora do dia
hourly_metrics = silver_df.groupBy(
    "pickup_hour",
    "pickup_dayname",
    "is_weekend",
    date_format("tpep_pickup_datetime", "yyyy-MM-dd").alias("trip_date")
).agg(
    count("*").alias("total_trips"),
    avg("total_amount").alias("avg_revenue_per_trip"),
    avg("trip_distance").alias("avg_distance"),
    avg("trip_duration_minutes").alias("avg_duration"),
    avg("tip_percentage").alias("avg_tip_percentage"),
    avg("avg_speed_mph").alias("avg_speed"),
    countDistinct("PULocationID").alias("unique_pickup_locations"),
    countDistinct("DOLocationID").alias("unique_dropoff_locations")
).withColumn("processing_timestamp", current_timestamp())

# Salvar tabela de m√©tricas hor√°rias
hourly_metrics.write \
    .mode("overwrite") \
    .partitionBy("is_weekend") \
    .format("delta") \
    .saveAsTable(GOLD_TABLES["trip_metrics_hourly"])

print(f"Tabela criada: {GOLD_TABLES['trip_metrics_hourly']}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 7. An√°lise de Localiza√ß√µes (Top Pickup/Dropoff)

# COMMAND ----------

print("Criando an√°lise de localiza√ß√µes...")

# M√©tricas por localiza√ß√£o de pickup
pickup_metrics = silver_df.groupBy("PULocationID").agg(
    count("*").alias("pickup_count"),
    avg("trip_distance").alias("avg_distance_from_location"),
    avg("total_amount").alias("avg_revenue_from_location"),
    avg("tip_percentage").alias("avg_tip_from_location"),
    countDistinct("DOLocationID").alias("unique_destinations")
).withColumn("location_type", lit("pickup"))

# M√©tricas por localiza√ß√£o de dropoff
dropoff_metrics = silver_df.groupBy("DOLocationID").agg(
    count("*").alias("dropoff_count"),
    avg("trip_distance").alias("avg_distance_to_location"),
    avg("total_amount").alias("avg_revenue_to_location"),
    avg("tip_percentage").alias("avg_tip_to_location"),
    countDistinct("PULocationID").alias("unique_origins")
).withColumn("location_type", lit("dropoff"))

# Combinar m√©tricas de localiza√ß√£o
location_metrics = pickup_metrics.select(
    col("PULocationID").alias("location_id"),
    "pickup_count", "avg_distance_from_location", 
    "avg_revenue_from_location", "avg_tip_from_location",
    "unique_destinations", "location_type"
).union(
    dropoff_metrics.select(
        col("DOLocationID").alias("location_id"),
        "dropoff_count", "avg_distance_to_location",
        "avg_revenue_to_location", "avg_tip_to_location", 
        "unique_origins", "location_type"
    )
).withColumn("processing_timestamp", current_timestamp())

# Salvar an√°lise de localiza√ß√µes
location_metrics.write \
    .mode("overwrite") \
    .partitionBy("location_type") \
    .format("delta") \
    .saveAsTable(GOLD_TABLES["location_metrics"])

print(f"Tabela criada: {GOLD_TABLES['location_metrics']}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 8. Performance por Vendor

# COMMAND ----------

print("Analisando performance por vendor...")

# M√©tricas detalhadas por vendor
vendor_performance = silver_df.groupBy(
    "VendorID", 
    "vendor_name",
    date_format("tpep_pickup_datetime", "yyyy-MM").alias("month_year")
).agg(
    # Volume e participa√ß√£o
    count("*").alias("total_trips"),
    
    # Performance financeira
    sum("total_amount").alias("total_revenue"),
    avg("total_amount").alias("avg_revenue_per_trip"),
    avg("fare_amount").alias("avg_fare"),
    avg("tip_amount").alias("avg_tip"),
    avg("tip_percentage").alias("avg_tip_percentage"),
    
    # Performance operacional
    avg("trip_distance").alias("avg_distance"),
    avg("trip_duration_minutes").alias("avg_duration"),
    avg("avg_speed_mph").alias("avg_speed"),
    
    # Qualidade do servi√ßo
    sum(when(col("has_outlier"), 1).otherwise(0)).alias("problematic_trips"),
    countDistinct("PULocationID").alias("coverage_pickup_locations"),
    countDistinct("DOLocationID").alias("coverage_dropoff_locations"),
    
    # Distribui√ß√£o por tipo de pagamento
    sum(when(col("payment_type") == 1, 1).otherwise(0)).alias("credit_card_trips"),
    sum(when(col("payment_type") == 2, 1).otherwise(0)).alias("cash_trips")
).withColumn(
    "market_share_trips", 
    col("total_trips") / sum("total_trips").over(Window.partitionBy("month_year")) * 100
).withColumn(
    "market_share_revenue",
    col("total_revenue") / sum("total_revenue").over(Window.partitionBy("month_year")) * 100
).withColumn(
    "outlier_rate", (col("problematic_trips") / col("total_trips")) * 100
).withColumn(
    "credit_card_rate", (col("credit_card_trips") / col("total_trips")) * 100
).withColumn("processing_timestamp", current_timestamp())

# Salvar performance por vendor
vendor_performance.write \
    .mode("overwrite") \
    .partitionBy("month_year") \
    .format("delta") \
    .saveAsTable(GOLD_TABLES["vendor_performance"])

print(f"Tabela criada: {GOLD_TABLES['vendor_performance']}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 9. An√°lise de M√©todos de Pagamento

# COMMAND ----------

print("üí≥ Analisando m√©todos de pagamento...")

# An√°lise detalhada por tipo de pagamento
payment_analysis = silver_df.groupBy(
    "payment_type",
    "payment_type_desc",
    "pickup_dayname",
    "time_period"
).agg(
    count("*").alias("total_trips"),
    sum("total_amount").alias("total_revenue"),
    avg("total_amount").alias("avg_amount"),
    avg("tip_amount").alias("avg_tip"),
    avg("tip_percentage").alias("avg_tip_percentage"),
    avg("fare_amount").alias("avg_fare"),
    stddev("tip_percentage").alias("tip_percentage_std")
).withColumn(
    "revenue_share",
    col("total_revenue") / sum("total_revenue").over(Window.partitionBy()) * 100
).withColumn("processing_timestamp", current_timestamp())

# Salvar an√°lise de pagamentos
payment_analysis.write \
    .mode("overwrite") \
    .format("delta") \
    .saveAsTable(GOLD_TABLES["payment_analysis"])

print(f"Tabela criada: {GOLD_TABLES['payment_analysis']}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 10. S√©rie Temporal para Forecasting

# COMMAND ----------

print("Criando s√©rie temporal...")

# S√©rie temporal com m√©tricas para forecasting
time_series = silver_df.groupBy(
    date_format("tpep_pickup_datetime", "yyyy-MM-dd HH:00:00").alias("datetime_hour"),
    hour("tpep_pickup_datetime").alias("hour_of_day"),
    dayofweek("tpep_pickup_datetime").alias("day_of_week"),
    "is_weekend"
).agg(
    count("*").alias("trips_count"),
    sum("total_amount").alias("revenue"),
    avg("trip_distance").alias("avg_distance"),
    avg("trip_duration_minutes").alias("avg_duration")
).withColumn(
    # Adicionar features para ML
    "hour_sin", sin(col("hour_of_day") * 2 * 3.14159 / 24)
).withColumn(
    "hour_cos", cos(col("hour_of_day") * 2 * 3.14159 / 24)
).withColumn(
    "day_sin", sin(col("day_of_week") * 2 * 3.14159 / 7)
).withColumn(
    "day_cos", cos(col("day_of_week") * 2 * 3.14159 / 7)
).withColumn("processing_timestamp", current_timestamp())

# Salvar s√©rie temporal
time_series.write \
    .mode("overwrite") \
    .partitionBy("is_weekend") \
    .format("delta") \
    .saveAsTable(GOLD_TABLES["time_series_metrics"])

print(f"Tabela criada: {GOLD_TABLES['time_series_metrics']}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 11. An√°lise de Rotas Populares

# COMMAND ----------

print("Analisando rotas populares...")

# Top rotas com an√°lise detalhada
route_analysis = silver_df.groupBy(
    "PULocationID", 
    "DOLocationID"
).agg(
    count("*").alias("trip_count"),
    sum("total_amount").alias("total_revenue"),
    avg("total_amount").alias("avg_revenue_per_trip"),
    avg("trip_distance").alias("avg_distance"),
    avg("trip_duration_minutes").alias("avg_duration"),
    avg("avg_speed_mph").alias("avg_speed"),
    avg("tip_percentage").alias("avg_tip_percentage"),
    stddev("total_amount").alias("revenue_std"),
    
    # Distribui√ß√£o temporal
    sum(when(col("is_weekend"), 1).otherwise(0)).alias("weekend_trips"),
    sum(when(col("time_period") == "Morning", 1).otherwise(0)).alias("morning_trips"),
    sum(when(col("time_period") == "Evening", 1).otherwise(0)).alias("evening_trips")
).withColumn(
    "weekend_ratio", col("weekend_trips") / col("trip_count") * 100
).withColumn(
    "revenue_per_mile", col("total_revenue") / col("avg_distance")
).withColumn(
    "route_efficiency_score", 
    (col("avg_speed") * col("avg_tip_percentage")) / col("avg_duration")
).withColumn("processing_timestamp", current_timestamp()) \
.orderBy(desc("trip_count"))

# Salvar an√°lise de rotas
route_analysis.write \
    .mode("overwrite") \
    .format("delta") \
    .saveAsTable(GOLD_TABLES["route_analysis"])

print(f"Tabela criada: {GOLD_TABLES['route_analysis']}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 12. Resumo Financeiro Executivo

# COMMAND ----------

print("üíº Criando resumo financeiro executivo...")

# Resumo financeiro por m√™s para executivos
financial_summary = silver_df.groupBy(
    "pickup_year",
    "pickup_month",
    date_format("tpep_pickup_datetime", "yyyy-MM").alias("month_year")
).agg(
    # M√©tricas de volume
    count("*").alias("total_trips"),
    countDistinct(date_format("tpep_pickup_datetime", "yyyy-MM-dd")).alias("active_days"),
    
    # Receitas
    sum("total_amount").alias("gross_revenue"),
    sum("fare_amount").alias("fare_revenue"),
    sum("tip_amount").alias("tip_revenue"),
    sum("tolls_amount").alias("tolls_revenue"),
    sum("extra").alias("extra_revenue"),
    
    # M√©tricas operacionais
    sum("trip_distance").alias("total_miles"),
    sum("trip_duration_minutes").alias("total_hours") / 60,
    sum("passenger_count").alias("total_passengers"),
    
    # M√©dias
    avg("total_amount").alias("avg_revenue_per_trip"),
    avg("trip_distance").alias("avg_miles_per_trip"),
    avg("tip_percentage").alias("avg_tip_rate")
).withColumn(
    "revenue_per_mile", col("gross_revenue") / col("total_miles")
).withColumn(
    "trips_per_day", col("total_trips") / col("active_days")
).withColumn(
    "revenue_per_day", col("gross_revenue") / col("active_days")
).withColumn(
    # Growth rates (compara√ß√£o m√™s anterior)
    "revenue_growth_rate",
    ((col("gross_revenue") - lag("gross_revenue").over(
        Window.orderBy("pickup_year", "pickup_month")
    )) / lag("gross_revenue").over(
        Window.orderBy("pickup_year", "pickup_month")
    )) * 100
).withColumn(
    "trips_growth_rate",
    ((col("total_trips") - lag("total_trips").over(
        Window.orderBy("pickup_year", "pickup_month")
    )) / lag("total_trips").over(
        Window.orderBy("pickup_year", "pickup_month")
    )) * 100
).withColumn("processing_timestamp", current_timestamp())

# Salvar resumo financeiro
financial_summary.write \
    .mode("overwrite") \
    .format("delta") \
    .saveAsTable(GOLD_TABLES["financial_summary"])

print(f"Tabela criada: {GOLD_TABLES['financial_summary']}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 13. Otimiza√ß√£o das Tabelas Gold

# COMMAND ----------

print("Otimizando tabelas Gold...")

# Otimizar cada tabela Gold
for table_name, table_path in GOLD_TABLES.items():
    try:
        print(f"   Otimizando {table_path}...")
        
        # OPTIMIZE com ZORDER apropriado para cada tabela
        if "daily" in table_name:
            spark.sql(f"OPTIMIZE {table_path} ZORDER BY (trip_date)")
        elif "hourly" in table_name:
            spark.sql(f"OPTIMIZE {table_path} ZORDER BY (pickup_hour, trip_date)")
        elif "vendor" in table_name:
            spark.sql(f"OPTIMIZE {table_path} ZORDER BY (VendorID, month_year)")
        elif "location" in table_name:
            spark.sql(f"OPTIMIZE {table_path} ZORDER BY (location_id)")
        elif "route" in table_name:
            spark.sql(f"OPTIMIZE {table_path} ZORDER BY (trip_count)")
        else:
            spark.sql(f"OPTIMIZE {table_path}")
            
        # Compute statistics
        spark.sql(f"ANALYZE TABLE {table_path} COMPUTE STATISTICS FOR ALL COLUMNS")
        
    except Exception as e:
        print(f"   Erro ao otimizar {table_path}: {str(e)}")

print("Otimiza√ß√£o conclu√≠da!")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 14. Dashboard de KPIs Principais

# COMMAND ----------

print("DASHBOARD DE KPIs PRINCIPAIS")
print("=" * 35)

# KPIs gerais do per√≠odo
overall_kpis = silver_df.agg(
    count("*").alias("total_trips"),
    sum("total_amount").alias("total_revenue"),
    sum("trip_distance").alias("total_distance"),
    sum("passenger_count").alias("total_passengers"),
    avg("total_amount").alias("avg_revenue_per_trip"),
    avg("trip_distance").alias("avg_distance_per_trip"),
    avg("trip_duration_minutes").alias("avg_duration_minutes"),
    avg("tip_percentage").alias("avg_tip_percentage"),
    countDistinct("PULocationID").alias("unique_pickup_locations"),
    countDistinct("DOLocationID").alias("unique_dropoff_locations")
).collect()[0]

print("üèÜ KPIs GERAIS (Jan-Abr 2023):")
print(f"   Total de viagens: {overall_kpis['total_trips']:,}")
print(f"   Receita total: ${overall_kpis['total_revenue']:,.2f}")
print(f"   Dist√¢ncia total: {overall_kpis['total_distance']:,.0f} milhas")
print(f"   Total de passageiros: {overall_kpis['total_passengers']:,.0f}")
print(f"   Receita m√©dia por viagem: ${overall_kpis['avg_revenue_per_trip']:.2f}")
print(f"   Dist√¢ncia m√©dia: {overall_kpis['avg_distance_per_trip']:.2f} milhas")
print(f"   Dura√ß√£o m√©dia: {overall_kpis['avg_duration_minutes']:.1f} minutos")
print(f"   Gorjeta m√©dia: {overall_kpis['avg_tip_percentage']:.1f}%")
print(f"   Locais de pickup √∫nicos: {overall_kpis['unique_pickup_locations']:,}")
print(f"   Locais de dropoff √∫nicos: {overall_kpis['unique_dropoff_locations']:,}")

# Top 5 dias com mais receita
print("\nTOP 5 DIAS COM MAIOR RECEITA:")
top_revenue_days = spark.table(GOLD_TABLES["trip_metrics_daily"]) \
    .select("trip_date", "total_revenue", "total_trips", "pickup_dayname") \
    .orderBy(desc("total_revenue")) \
    .limit(5)

top_revenue_days.show(truncate=False)

# Performance por vendor
print("\nPERFORMANCE POR VENDOR (Total do per√≠odo):")
vendor_summary = spark.table(GOLD_TABLES["vendor_performance"]) \
    .groupBy("vendor_name") \
    .agg(
        sum("total_trips").alias("total_trips"),
        sum("total_revenue").alias("total_revenue"),
        avg("avg_tip_percentage").alias("avg_tip_pct")
    ).orderBy(desc("total_revenue"))

vendor_summary.show()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 15. An√°lises Avan√ßadas e Insights

# COMMAND ----------

print("AN√ÅLISES AVAN√áADAS E INSIGHTS")
print("=" * 35)

# 1. An√°lise de sazonalidade por hora
print("\nPADR√ÉO DE DEMANDA POR HORA:")
hourly_demand = spark.table(GOLD_TABLES["trip_metrics_hourly"]) \
    .groupBy("pickup_hour") \
    .agg(
        avg("total_trips").alias("avg_trips_per_hour"),
        avg("avg_revenue_per_trip").alias("avg_revenue")
    ).orderBy("pickup_hour")

hourly_demand.show(24)

# 2. An√°lise de fins de semana vs dias √∫teis
print("\nFINS DE SEMANA vs DIAS √öTEIS:")
weekend_comparison = spark.table(GOLD_TABLES["trip_metrics_daily"]) \
    .groupBy("is_weekend") \
    .agg(
        avg("total_trips").alias("avg_daily_trips"),
        avg("total_revenue").alias("avg_daily_revenue"),
        avg("avg_tip_percentage").alias("avg_tip_pct")
    )

weekend_comparison.show()

# 3. Top 10 rotas mais lucrativas
print("\nTOP 10 ROTAS MAIS LUCRATIVAS:")
top_profitable_routes = spark.table(GOLD_TABLES["route_analysis"]) \
    .select(
        "PULocationID", "DOLocationID", "trip_count", 
        "total_revenue", "avg_revenue_per_trip", "revenue_per_mile"
    ) \
    .orderBy(desc("total_revenue")) \
    .limit(10)

top_profitable_routes.show()

# 4. An√°lise de crescimento mensal
print("\nCRESCIMENTO MENSAL:")
monthly_growth = spark.table(GOLD_TABLES["financial_summary"]) \
    .select(
        "month_year", "total_trips", "gross_revenue", 
        "trips_growth_rate", "revenue_growth_rate"
    ) \
    .orderBy("month_year")

monthly_growth.show()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 16. Cria√ß√£o de Views para Dashboards

# COMMAND ----------

print("Criando views para dashboards...")

# View consolidada de KPIs principais
spark.sql(f"""
CREATE OR REPLACE VIEW gold.kpi_dashboard AS
SELECT 
    trip_date,
    pickup_dayname,
    is_weekend,
    total_trips,
    total_revenue,
    avg_fare_amount,
    avg_tip_percentage,
    avg_trip_distance,
    avg_trip_duration,
    revenue_per_mile,
    processing_timestamp
FROM {GOLD_TABLES["trip_metrics_daily"]}
ORDER BY trip_date
""")

# View de tend√™ncias hor√°rias
spark.sql(f"""
CREATE OR REPLACE VIEW gold.hourly_trends AS
SELECT 
    pickup_hour,
    pickup_dayname,
    is_weekend,
    AVG(total_trips) as avg_trips,
    AVG(avg_revenue_per_trip) as avg_revenue,
    AVG(avg_tip_percentage) as avg_tip_pct
FROM {GOLD_TABLES["trip_metrics_hourly"]}
GROUP BY pickup_hour, pickup_dayname, is_weekend
ORDER BY pickup_hour, pickup_dayname
""")

# View de performance de vendors
spark.sql(f"""
CREATE OR REPLACE VIEW gold.vendor_dashboard AS
SELECT 
    vendor_name,
    month_year,
    total_trips,
    total_revenue,
    market_share_trips,
    market_share_revenue,
    avg_tip_percentage,
    outlier_rate
FROM {GOLD_TABLES["vendor_performance"]}
ORDER BY month_year, total_revenue DESC
""")

print("Views criadas para dashboards")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 17. Valida√ß√£o e Teste das Tabelas Gold

# COMMAND ----------

print("VALIDA√á√ÉO DAS TABELAS GOLD")
print("=" * 30)

# Verificar todas as tabelas criadas
for table_name, table_path in GOLD_TABLES.items():
    try:
        df = spark.table(table_path)
        count = df.count()
        columns = len(df.columns)
        print(f"{table_name}: {count:,} registros, {columns} colunas")
        
        # Verificar se h√° dados nulos nas colunas principais
        if "total_trips" in df.columns:
            null_trips = df.filter(col("total_trips").isNull()).count()
            if null_trips > 0:
                print(f"   {null_trips} registros com total_trips nulo")
                
    except Exception as e:
        print(f"Erro na tabela {table_name}: {str(e)}")

# Verificar consist√™ncia entre tabelas
print(f"\nVERIFICA√á√ÉO DE CONSIST√äNCIA:")

# Total de viagens deve ser consistente
daily_total = spark.table(GOLD_TABLES["trip_metrics_daily"]).agg(sum("total_trips")).collect()[0][0]
silver_total = silver_df.count()

print(f"   Viagens na Silver: {silver_total:,}")
print(f"   Soma das m√©tricas di√°rias: {daily_total:,}")
print(f"   Diferen√ßa: {abs(silver_total - daily_total):,}")

if abs(silver_total - daily_total) < 100:  # Toler√¢ncia pequena
    print("   Consist√™ncia validada")
else:
    print("   Poss√≠vel inconsist√™ncia detectada")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 18. Relat√≥rio Final de M√©tricas Gold

# COMMAND ----------

print("RELAT√ìRIO FINAL - CAMADA GOLD")
print("=" * 35)

print("Tabelas Gold criadas com sucesso:")
for i, (table_name, table_path) in enumerate(GOLD_TABLES.items(), 1):
    print(f"   {i}. {table_path}")

print(f"\nM√©tricas principais extra√≠das:")
print("   ‚Ä¢ M√©tricas di√°rias e hor√°rias de opera√ß√£o")
print("   ‚Ä¢ Performance detalhada por vendor")
print("   ‚Ä¢ An√°lise de localiza√ß√µes e rotas populares")
print("   ‚Ä¢ Padr√µes de pagamento e gorjetas")
print("   ‚Ä¢ S√©ries temporais para forecasting")
print("   ‚Ä¢ Resumos financeiros executivos")

print(f"\nKPIs dispon√≠veis para dashboards:")
for kpi in MAIN_KPIS:
    print(f"   ‚Ä¢ {kpi}")

print(f"\nViews criadas para consumo:")
print("   ‚Ä¢ gold.kpi_dashboard - KPIs principais")
print("   ‚Ä¢ gold.hourly_trends - Tend√™ncias hor√°rias")
print("   ‚Ä¢ gold.vendor_dashboard - Performance vendors")

print(f"\nOtimiza√ß√µes aplicadas:")
print("   ‚Ä¢ OPTIMIZE e ZORDER em todas as tabelas")
print("   ‚Ä¢ Particionamento estrat√©gico")
print("   ‚Ä¢ Estat√≠sticas computadas para otimiza√ß√£o de queries")
print("   ‚Ä¢ √çndices impl√≠citos do Delta Lake")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Resumo da Camada Gold
# MAGIC 
# MAGIC ### Tabelas Anal√≠ticas Criadas:
# MAGIC 1. **M√©tricas Di√°rias** - KPIs consolidados por dia
# MAGIC 2. **M√©tricas Hor√°rias** - An√°lise de padr√µes temporais
# MAGIC 3. **An√°lise de Localiza√ß√µes** - Performance por pickup/dropoff
# MAGIC 4. **Performance de Vendors** - Comparativo entre fornecedores
# MAGIC 5. **An√°lise de Pagamentos** - Padr√µes por tipo de pagamento
# MAGIC 6. **S√©ries Temporais** - Dados preparados para forecasting
# MAGIC 7. **An√°lise de Rotas** - Rotas mais populares e lucrativas
# MAGIC 8. **Resumo Financeiro** - M√©tricas executivas mensais
# MAGIC 
# MAGIC ### KPIs Principais Dispon√≠veis:
# MAGIC - Volume de viagens e crescimento
# MAGIC - Receitas e rentabilidade
# MAGIC - M√©tricas operacionais (dist√¢ncia, dura√ß√£o, velocidade)
# MAGIC - Satisfa√ß√£o do cliente (gorjetas)
# MAGIC - Cobertura geogr√°fica
# MAGIC - Qualidade dos dados
# MAGIC 
# MAGIC ### Casos de Uso Habilitados:
# MAGIC - **Dashboards Executivos** - M√©tricas de alto n√≠vel
# MAGIC - **An√°lise Operacional** - Otimiza√ß√£o de rotas e hor√°rios
# MAGIC - **Forecasting** - Previs√£o de demanda
# MAGIC - **An√°lise Competitiva** - Performance entre vendors
# MAGIC - **Business Intelligence** - Insights para tomada de decis√£o
# MAGIC 
# MAGIC ### Pr√≥ximos Passos:
# MAGIC 1. Executar notebook `04_automatizacao.py` para pipeline automatizado
# MAGIC 2. Conectar ferramentas de BI (Tableau, Power BI)
# MAGIC 3. Implementar alertas e monitoramento
# MAGIC 4. Desenvolver modelos de ML para forecasting
# MAGIC 
# MAGIC ### Performance e Escalabilidade:
# MAGIC - Todas as tabelas otimizadas com ZORDER
# MAGIC - Particionamento estrat√©gico para consultas r√°pidas
# MAGIC - Views pr√©-configuradas para dashboards
# MAGIC - Delta Lake para ACID transactions e time travel

# COMMAND ----------

print("AN√ÅLISE GOLD CONCLU√çDA COM SUCESSO!")
print("Pr√≥ximo passo: Execute o notebook 04_automatizacao.py")
print("As tabelas Gold est√£o prontas para consumo em dashboards e relat√≥rios!")
